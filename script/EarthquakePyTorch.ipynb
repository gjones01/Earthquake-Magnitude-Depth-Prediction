{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating A Feedforward Neural Network In PyTorch To Predict Earthquake Depth Ranges*\n",
    "\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from imblearn.over_sampling import SMOTE\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load the cleaned csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "clean_eq = pd.read_csv('/Users/gerryjr/Desktop/EarthquakeProject/Data/clean_earthquake.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before getting into the process of filling/cleaning any missing data, let's create ranges(bins) to categorize earthquake depths. Concurrently, let's map the \"labels\" which are strings. This gives the strings of \"shallow\", \"intermediate\" and \"deep\" a numeric label of 0, 1 and 2. Since this will be a Feedforward Neural Network, the model needs the inputs to be numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create depth ranges\n",
    "clean_eq['depth_range'] = pd.cut(clean_eq['depth'], bins=[0, 70, 300, float('inf')], labels=['Shallow', 'Intermediate', 'Deep'])\n",
    "\n",
    "# Map depth ranges to numeric labels\n",
    "depth_mapping = {'Shallow': 0, 'Intermediate': 1, 'Deep': 2}\n",
    "clean_eq['depth_range'] = clean_eq['depth_range'].map(depth_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, this drops any rows in the \"depth_range\" column that has missing or non-numeric values. This will be important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "clean_eq = clean_eq.dropna(subset=['depth_range'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like the TensorFlow FNN, features are engineered. The follow feature engineering is related to depth which will be important for the model to train from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/_2/d_jf9l7x2rl2fp7yfmdp4c100000gn/T/ipykernel_3742/2690894172.py:4: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clean_eq['depth_error_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
      "/var/folders/_2/d_jf9l7x2rl2fp7yfmdp4c100000gn/T/ipykernel_3742/2690894172.py:5: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  clean_eq['depth_error_ratio'].fillna(clean_eq['depth_error_ratio'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "#Feature Engineering\n",
    "clean_eq['depth_squared'] = clean_eq['depth'] ** 2\n",
    "clean_eq['depth_error_ratio'] = clean_eq['depthError'] / clean_eq['depth']\n",
    "clean_eq['depth_error_ratio'].replace([np.inf, -np.inf], np.nan, inplace=True)\n",
    "clean_eq['depth_error_ratio'].fillna(clean_eq['depth_error_ratio'].median(), inplace=True)\n",
    "clean_eq['lat_long_interaction'] = clean_eq['latitude'] * clean_eq['longitude']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The features are listed with the fixed and engineered variants. In this case, the target is \"depth_range.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and target\n",
    "X = clean_eq[['depth',\n",
    "              'gap',\n",
    "              'rms',\n",
    "              'depth_squared',\n",
    "              'latitude',\n",
    "              'longitude',\n",
    "              'depth_error_ratio',\n",
    "              'lat_long_interaction']]\n",
    "y = clean_eq['depth_range']\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like other models, this splits the testing and training data with 20% being set aside for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "X_train = X_train.fillna(X_train.mean())  # or .median()\n",
    "X_test = X_test.fillna(X_test.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since this is a neural network, it is important to scale the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below cell implenets SMOTE which stand sfor Synthetic Minority Over-Sampling Technique. This allows the model to deal with class imbalances in the data. In the previous Jupyter Notebook you will notice the distribution plot showed am imbalance of earthquakes of magnitudes ~5. This means that the model can encounter an issue with identifying outliers (aka minorities). I added this cell afterwards when the model was struggling to identify intermidiate and deep depth ranges. This will become more apparent when the results are shown for the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/base.py:474: FutureWarning: `BaseEstimator._validate_data` is deprecated in 1.6 and will be removed in 1.7. Use `sklearn.utils.validation.validate_data` instead. This function becomes public and is part of the scikit-learn developer API.\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/sklearn/utils/_tags.py:354: FutureWarning: The SMOTE or classes from which it inherits use `_get_tags` and `_more_tags`. Please define the `__sklearn_tags__` method, or inherit from `sklearn.base.BaseEstimator` and/or other appropriate mixins such as `sklearn.base.TransformerMixin`, `sklearn.base.ClassifierMixin`, `sklearn.base.RegressorMixin`, and `sklearn.base.OutlierMixin`. From scikit-learn 1.7, not defining `__sklearn_tags__` will raise an error.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE\n",
    "smote = SMOTE(sampling_strategy='auto', random_state=42)\n",
    "X_train, y_train = smote.fit_resample(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While this looks a bit abstract, these 4 lines simply tell the model that 32-bit floating are the inputs. 32-floating point numbers are standard procedure for deep learning models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train.values, dtype=torch.long)\n",
    "y_test = torch.tensor(y_test.values, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to create the FNN. Since this is utilizing PyTorch the syntax. Since PyTorch runs the computations dynamically, the code is slightly more rigorous than TensorFlow. However, the overall methodology does not change. A dropout of 40% is added to force the model to learn patterns instead of memorizing the data. The \"relu\" activation function is added for non-linearity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FNN with dropout\n",
    "class FNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super(FNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.4)  # Dropout to reduce overfitting\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the function with the layers and size of said layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model\n",
    "input_size = X_train.shape[1]\n",
    "hidden_size = 128  # Adjusted hidden layer size\n",
    "output_size = len(depth_mapping)\n",
    "model = FNN(input_size, hidden_size, output_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The loss and optimization are defined. This measures how accurate the predictions are and modifies how the model adjusts the neuron weights during the backpropagation portion of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function and optimizer \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=0.001)  # L2 regularization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time to train the model. The iterations (epochs) are set as 50. This is written as a loop for each iteration. Again PyTorch computes dynamically so there is more flexibility in the structure of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/50], Loss: 0.7998\n",
      "Epoch [20/50], Loss: 0.6824\n",
      "Epoch [30/50], Loss: 0.5929\n",
      "Epoch [40/50], Loss: 0.5238\n",
      "Epoch [50/50], Loss: 0.4664\n"
     ]
    }
   ],
   "source": [
    "# Training the FNN\n",
    "num_epochs = 50\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    loss = criterion(outputs, y_train)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the model is trained, now it is important to evaluate how well it performed. This prints the accuracy and a classification report. This will discriminate the 3 sub-targets(shallow, intermediate and deep) to evaluate how accurately the model predicted them. To have a visual correlation, a \"Confusion Matrix\" is generate to show how well the model did."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9565\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     Shallow       0.98      0.97      0.98      1271\n",
      "Intermediate       0.83      0.81      0.82       189\n",
      "        Deep       0.95      1.00      0.98        80\n",
      "\n",
      "    accuracy                           0.96      1540\n",
      "   macro avg       0.92      0.93      0.92      1540\n",
      "weighted avg       0.96      0.96      0.96      1540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1239   32    0]\n",
      " [  31  154    4]\n",
      " [   0    0   80]]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    test_outputs = model(X_test)\n",
    "    _, predicted = torch.max(test_outputs, 1)\n",
    "    accuracy = accuracy_score(y_test, predicted)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    print(\"Classification Report:\")\n",
    "    print(classification_report(y_test, predicted, target_names=['Shallow', 'Intermediate', 'Deep']))\n",
    "\n",
    "    #Confusion matrix\n",
    "    cm = confusion_matrix(y_test, predicted)\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(cm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above output indicates the model is performing quite well, consistently performing in the low to middle 90%. The \"shallow\" class performed at 98% which is very strong. The intermidiate class had some missclassifications at 70%. This is somewhat expected since the model is conflating scenarios that are close to the \"deep\" class. Finally, the \"deep\" class performed at 88% which is respectable. Overall the model performs well."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
