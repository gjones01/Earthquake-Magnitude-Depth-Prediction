{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Creating A Feedforward Neural Network In TensorFlow To Predict Earthquake Magnitudes*\n",
    "\n",
    "\n",
    "Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial dataset was extremely messy, including misaligned columns. Thus, missing data was't filled but rather aligned. Then the new csv \"clean_earthquake\" was read in using pandas. In doing so, the necessary null values were filled. There was no need for encoding any of the columns since they would not be relevant features for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_eq = pd.read_csv('/Users/gerryjr/Desktop/EarthquakeProject/Data/clean_earthquake.csv')\n",
    "\n",
    "clean_eq['gap'] = clean_eq['gap'].fillna(clean_eq['gap'].median())\n",
    "clean_eq['dmin'] = clean_eq['dmin'].fillna(clean_eq['dmin'].mean())\n",
    "clean_eq['rms'] = clean_eq['rms'].fillna(clean_eq['rms'].mean())\n",
    "clean_eq['depthError'] = clean_eq['depthError'].fillna(clean_eq['depthError'].median())\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a pre-model visualization to give an idea of how the magnitudes are distributed across their frequency in occurence. It is always good to have a visual before creating the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(clean_eq['mag'], kde=True, bins=30, color='blue')\n",
    "plt.title('Distribution of Earthquake Magnitudes')\n",
    "plt.xlabel('Magnitude')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is a geo-spatial visual, created in Tableau, aligning the latitude and longitude coordinates. This provides an interactive visual for the geographical location and color gradient based on magnitude. Each data point is labeled with the corresponding latitude, longitude, time, depth and magnitude.\n",
    "\n",
    "[View Interactive Tableau Visualization](https://public.tableau.com/views/GeoSpacial_17382574271160/Geo-SPacialVisualization?:language=en-US&publish=yes&:sid=&:redirect=auth&:display_count=n&:origin=viz_share_link)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it is time for selecting features and engineering any necessary ones. As mentioned before, portions of data had such severe amounts of null space that they were eliminated completely. Thus, 3 additional features are engineered to aid the model in drawing more information for the magnitude prediction. In particular, the 'depth_error_ratio' is a relationship feature that can provide meaningful information. The final engineered feature is the Gutenberg-Richter relation. After doing some research online, it became clear that this relation in geology, takes the magnitude of earthquakes and the number of earthquakes of that magnitude or greater. After trial and error with feature engineering, this one in particular made a significant difference in the model accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering\n",
    "clean_eq['depth_squared'] = clean_eq['depth'] ** 2\n",
    "clean_eq['depth_error_ratio'] = clean_eq['depthError'] / clean_eq['depth']\n",
    "clean_eq['depth_error_ratio'].replace([np.inf, -np.inf], np.nan)\n",
    "clean_eq['depth_error_ratio'].fillna(clean_eq['depth_error_ratio'].median())\n",
    "\n",
    "#Gutenberg-Richter Relationship\n",
    "clean_eq['energy_release'] = 10 ** (1.5 *clean_eq['mag'])\n",
    "clean_eq['energy_release'] = np.clip(clean_eq['energy_release'], None, 1e10)  # Adjust threshold as needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the features for the model to learn from. Standard features such as, depth, rms and gap or entered along with the engineered features mentioned in the previous markdown. Obviously, the target for this scenario is magnitude('mag'). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features and Targets\n",
    "X = clean_eq[['latitude',\n",
    "            'longitude', \n",
    "            'depth', \n",
    "            'gap', \n",
    "            'dmin', \n",
    "            'rms', \n",
    "            'depthError', \n",
    "            'depth_squared', \n",
    "            'depth_error_ratio', \n",
    "            'energy_release'\n",
    "            ]]\n",
    "y = clean_eq['mag']  # Target variable (magnitude)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before splitting the training and testing, it is important to scale the features.\n",
    "\n",
    "The train/test split is initiated with a testing size of 20%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Train/Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, it is time to assemble the Feedforward Neural Network. Keep in mind that Sequential, Dense and Dropout were imported from tensorflow.keras.layer in the first cell and there are 4 layers to this model. When this was initially created, there was overfitting (\"the model accuracy being 100%\") which is not a good thing. That means the model is relying too heavily on particular neurons or simply memorizing portions of the trained data. The goal is to have the model recognize patterns so it can make accurate predictions with future data, not memorize the current data. Therefore, a Dropout function is placed between each layer where it drops 20% of the neurons in each training step. This forces the model to learn, not memorize. The final layer has one neuron since it is the goal to predict one target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build Feedforward Neural Network (FNN)\n",
    "fnn = Sequential([\n",
    "    Dense(128, input_dim=X_train.shape[1], activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(64,activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(1)  # Single output neuron for magnitude\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the model is built, it is time to compile, train, evaluate and predict. The compiler tells the neural network how to learn the data. In this case the \"adam\" optimizer argument is common and dynamically updates the weights through training. The loss argument ('mse') measures how far off the predictions are from the real values. The metrics argument ('mae') calculates the absolute distance between predictions and true values. \n",
    "\n",
    "After compiling, the model gets trained with 50 epochs (iterations).\n",
    "\n",
    "Finally, it is time to predict the tested X values. In doing so, we print the first 10 predicted values along with the first 10 actual values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the FNN\n",
    "fnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "# Train the FNN\n",
    "history = fnn.fit(X_train, y_train, epochs=50, batch_size=10, validation_split=0.2)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, mae = fnn.evaluate(X_test, y_test)\n",
    "print(f\"Loss: {loss}, Mean Absolute Error: {mae}\")\n",
    "\n",
    "# Make predictions for the entire test set\n",
    "pred = fnn.predict(X_test)\n",
    "\n",
    "# Print the first 10 predictions and true values for verification\n",
    "print(\"First 10 Predictions:\", pred[:10].flatten())\n",
    "print(\"First 10 True Values:\", y_test[:10].values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^You will notice, as the above cell ran, the \"loss\" dropped each iteration which is good. This indicates the model became more accurate each iteration. The output also indicates a fairly good prediction.\n",
    "\n",
    "Now let's display the results of the MSE and MAE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate metrics for the entire test set\n",
    "mae = mean_absolute_error(y_test, pred)\n",
    "mse = mean_squared_error(y_test, pred)\n",
    "print(\"Mean Absolute Error (MAE):\", mae)\n",
    "print(\"Mean Squared Error (MSE):\", mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "Now prepare the data for a visualization in Tableau, specifically the relationship between depth and error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare the data for Tableau visualization\n",
    "X_test = pd.DataFrame(X_test, columns=[['latitude',\n",
    "            'longitude', \n",
    "            'depth', \n",
    "            'gap', \n",
    "            'dmin', \n",
    "            'rms', \n",
    "            'depthError', \n",
    "            'depth_squared', \n",
    "            'depth_error_ratio', \n",
    "            'energy_release'\n",
    "            ]])\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "pred_series = pd.Series(pred.flatten(), name=\"Predicted_Magnitude\", index=y_test.index)\n",
    "\n",
    "# Add True Magnitude and Predicted Magnitude to the DataFrame\n",
    "X_test['True_Magnitude'] = y_test\n",
    "X_test['Predicted_Magnitude'] = pred_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After preparing the data, it is brought into Tableau and visualized in a density plot. This plot indicates that error is lower for shallower depths and more innacurate as they become deeper. This is somewhat expected. This goes back to my previous point on having more reliable data for more accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create a DataFrame with true and predicted values\n",
    "results_df = pd.DataFrame({\n",
    "    'True Magnitude': y_test.values.flatten(),\n",
    "    'Predicted Magnitude': pred.flatten()})\n",
    "# Save to CSV\n",
    "results_df.to_csv('true_vs_predicted.csv', index=False)\n",
    "\n",
    "print(\"CSV file 'true_vs_predicted.csv' created successfully!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having saved the csv, the below visualization is a scatter plot of the true and predicted magnitudes\n",
    "\n",
    "[View Tableau Visualization](https://public.tableau.com/views/TrueVSPredictedMagnitudes/TrueVSPredictedMagnitudes)\n",
    "\n",
    "\n",
    "The scatter plot above compares the true magnitudes of recorded earthquakes with the magnitudes predicted by the trained neural network. The diagonal polynomial reference line indicates perfect predictions—points. Points that deviate from this line show scenarios where the model miscalculated the true magnitude. Overall, the model performs well for most cases, particularly for moderate magnitudes. There are higher errors for stronger earthquakes, which suggests potential areas for improvement, such as additional feature engineering or refining the model’s architecture.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
